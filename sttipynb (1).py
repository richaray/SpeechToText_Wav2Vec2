# -*- coding: utf-8 -*-
"""STTipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PVPKbwnB8mAi7YE_FZ8VJ1KM4p7tui0o
"""

! pip install -q kaggle

from google.colab import drive
drive.mount('/content/drive')

! mkdir ~/.kaggle

! cp /content/drive/MyDrive/Kaggle_API/kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download mathurinache/the-lj-speech-dataset

! unzip the-lj-speech-dataset.zip

!pip install -q transformers
!pip install -q torchmetrics

import pandas as pd
import numpy as np
import random
import torch
import os
import librosa
from IPython.display import Audio
import re
from torchmetrics import WordErrorRate

"""# Read File"""

data_dir = '/content/LJSpeech-1.1/wavs'

file_paths = []
for root, direc, files in os.walk(data_dir):
    for filename in files:
        file_path = os.path.join(root,filename)
        file_paths.append(file_path)

file_paths[0:5]

"""EDA"""

#file is not sorted so lets sorted it out
file_paths_sorted = sorted(file_paths)
file_paths_sorted[0:5]

df = pd.read_csv('/content/LJSpeech-1.1/metadata.csv', sep='|', names=["File", "Text", "Text2"])
df.head()

#we only need 1 Text
df = df.drop(['Text2'], axis=1)
#put path location to Dataframe
df['path'] = file_paths_sorted
df.head()

#function to play a video
def palay(dir, num):
    return Audio(dir['path'][num], autoplay=True)

palay(df, 0)

"""Preprocess Data"""

#remove special character
def remove_special_characters(data, column_name):
    regex = r'[^a-zA-Z0-9\s]'
    data[column_name] = data[column_name].str.replace(regex,'').str.lower()
    return data

df = remove_special_characters(df, 'Text')
df.head()

"""Audio Transcription"""

import torch
from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = Wav2Vec2Tokenizer.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h").to(device)

def transcript(data):
    #load recording
    audio, rate = librosa.load(data['path'], sr = 16000)

    # taking input value
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    input_values = tokenizer(audio, return_tensors = "pt").input_values.to(device)
    #sore logits
    logits = model(input_values).logits

    #storing predicted ids
    prediction = torch.argmax(logits, dim =-1)
    #transcription text
    data['transcription'] = tokenizer.batch_decode(prediction)[0]
    return data

result = df.apply(transcript, axis=1)
result.head()

result.rename(columns= {'Transcription':'Text'}, inplace= True)
result.to_csv('/content/results.csv', index=False)

result.shape

"""Evaluation"""

def wer(file):
    we = WordErrorRate()
    file['wer']=int(we(file['transcription'].lower(), file['Text'])*100)
    return file

result = result.apply(wer, axis=1)
result['wer'].head()

average = result['wer'].mean()
print(f'WORD ERROR RATE = {average}%')

result.to_csv('/content/results.csv',index=False)

result.head()

"""Prediction"""

from google.colab import files
import librosa

# Function to transcribe a single audio file
def transcribe_single_audio(file_path):
    # Load the audio file
    audio, rate = librosa.load(file_path, sr=16000)

    # Prepare input values for the model
    input_values = tokenizer(audio, return_tensors="pt").input_values.to(device)

    # Get logits from the model
    logits = model(input_values).logits

    # Decode predictions
    prediction = torch.argmax(logits, dim=-1)
    transcription = tokenizer.batch_decode(prediction)[0]

    return transcription

# Upload an audio file
uploaded = files.upload()

# Process the uploaded file (assuming a single file is uploaded)
for file_name in uploaded.keys():
    transcription = transcribe_single_audio(file_name)
    print(f"Transcription for '{file_name}':\n{transcription}")

